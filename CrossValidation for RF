import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score
import xgboost as xgb

file_path = r"C:\Users\brand\Desktop\PyCharm Community Edition 2024.3\Biomarkers\AD_combined_filtered_species.csv"
AD_combined_filtered = pd.read_csv(file_path)

# Display the rows of the dataset
print(AD_combined_filtered.head(30))
print(AD_combined_filtered.columns)
print(AD_combined_filtered['label'].value_counts())

# Define features (X) and target (y)
X = AD_combined_filtered.drop(columns=['label', 'SubjectID','Source'])  # Correct
# Check data types
print(X.dtypes)
y = AD_combined_filtered['label']                # Use 'label' as target

#X, y = make_classification(n_samples= 1000, n_features= 20, n_informative=15,n_redundant=5)
# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.2, random_state=42)
# Check shapes
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

print("After SMOTE:", pd.Series(y_train_sm).value_counts())

# Define a parameter grid for RandomForestClassifier
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node
    'max_features': ['sqrt', 'log2', None],  # Number of features to consider for the best split
    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees
}
# Train a RandomForest baseline model
rf_classifier = RandomForestClassifier(random_state=42)
# Use GridSearchCV to search for the best parameters
grid_search = GridSearchCV(
    estimator=rf_classifier,
    param_grid=param_grid,
    scoring='accuracy',  # Evaluation metric
    cv=5,  # Number of cross-validation folds
    verbose=2,  # Verbosity level
    n_jobs=-1  # Use all available cores
)
# Fit the grid search to the resampled training data
print("Starting GridSearchCV...\n")
grid_search.fit(X_train_sm, y_train_sm)

# Extract the best parameters and the best model
best_params = grid_search.best_params_
best_rf_model = grid_search.best_estimator_


# Print the best parameters
print("Best Parameters:\n", best_params)

# Evaluate the best model on the test set
y_pred_best = best_rf_model.predict(X_test)

# Create the confusion matrix for the best model
conf_matrix_best = confusion_matrix(y_test, y_pred_best)
rf_classifier.fit(X_train_sm, y_train_sm)
y_pred = rf_classifier.predict(X_test)
# Create the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
# Display the confusion matrix
print("Confusion Matrix:\n", conf_matrix)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Extract feature importances
feature_importances = rf_classifier.feature_importances_
features = X.columns  # Column names

# Create a DataFrame for sorting and visualization
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Display the top 10 features
print("Top 10 Features Linked to AD:\n", importance_df.head(10))

# Cross Validation 
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score

# File path to the dataset
file_path = r"C:\Users\brand\Desktop\PyCharm Community Edition 2024.3\Biomarkers\AD_combined_filtered_species.csv"

# Load the dataset
AD_combined_filtered = pd.read_csv(file_path)

# Display dataset details
print("First 30 rows of the dataset:")
print(AD_combined_filtered.head(30))
print("\nColumn names:")
print(AD_combined_filtered.columns)
print("\nLabel distribution:")
print(AD_combined_filtered['label'].value_counts())

# Define features (X) and target (y)
X = AD_combined_filtered.drop(columns=['label', 'SubjectID', 'Source'])
y = AD_combined_filtered['label']

# Check data types
print("\nFeature data types:")
print(X.dtypes)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("\nTraining set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

# Apply SMOTE for oversampling
smote = SMOTE(random_state=42)
X_sm, y_sm = smote.fit_resample(X, y)
print("\nLabel distribution after SMOTE:")
print(pd.Series(y_sm).value_counts())

# Initialize the optimized RandomForestClassifier
optimized_rf_classifier = RandomForestClassifier(
    bootstrap=True,
    max_depth=10,
    max_features='sqrt',
    min_samples_leaf=2,
    min_samples_split=5,
    n_estimators=100,
    random_state=42
)

# Perform Stratified K-Fold cross-validation
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(optimized_rf_classifier, X_sm, y_sm, cv=stratified_kfold, scoring='accuracy')

# Display cross-validation results
print("\nCross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Accuracy:", np.mean(cv_scores))

# Display fold details
for fold_idx, (train_indices, test_indices) in enumerate(stratified_kfold.split(X, y)):
    print(f"\nFold {fold_idx + 1}:")
    print("Train Indices:", train_indices)
    print("Test Indices:", test_indices)
    print("Test Data:\n", X.iloc[test_indices])
    print("Labels:\n", y.iloc[test_indices])

# Train the optimized model on the entire dataset after SMOTE
optimized_rf_classifier.fit(X_sm, y_sm)

# Extract and display feature importances
feature_importances_optimized = optimized_rf_classifier.feature_importances_
importance_df_optimized = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances_optimized
}).sort_values(by='Importance', ascending=False)

print("\nTop 10 Features Linked to AD (Optimized Model):")
print(importance_df_optimized.head(10))







